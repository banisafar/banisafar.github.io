<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A deep dive into LoRA (Low-Rank Adaptation) - mathematical foundations and practical implementation">
    <meta name="author" content="Sahar Banisafar">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Franklin:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <title>LoRA: Low-Rank Adaptation for Efficient Fine-Tuning</title>
    
    <!-- Math rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;
            
            font-size: 18px;
            line-height: 1.7;
            color: #333;
            background: #fff;
        }
        
        /* Header styles */
        header {
            border-bottom: 1px solid #e0e0e0;
            padding: 20px 0;
            margin-bottom: 40px;
        }
        
        .simple-header {
            max-width: 750px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .site-name {
            font-size: 20px;
            font-weight: 700;
            color: #333;
            text-decoration: none;
        }
        /*
        .site-name {
            font-family: 'Libre Franklin', sans-serif;  /* ADD THIS */
            font-size: 20px;
            font-weight: 700;
            color: #333;
            text-decoration: none;
        }
        */
        
        .site-name:hover {
            color: #0066cc;
        }
        
        header nav {
            display: flex;
            gap: 20px;
        }
        
        header nav a {
            color: #666;
            text-decoration: none;
            font-size: 16px;
        }
        
        header nav a:hover {
            color: #0066cc;
        }
        
        /* Main content */
        main {
            max-width: 750px;
            margin: 0 auto;
            padding: 0 20px 80px;
        }
        
        h1 {
            font-family: 'Libre Franklin', sans-serif;
            font-size: 2.2em;
            font-weight: 600;
            margin-bottom: 0.3em;
            line-height: 1.2;
        }
        
        .subtitle {
            font-style: italic;
            color: #666;
            margin-bottom: 2em;
        }
        
        h2 {
            font-family: 'Libre Franklin', sans-serif;  /* ADD THIS */
            font-size: 1.6em;
            font-weight: 600;
            margin-top: 2em;
            margin-bottom: 0.8em;
            padding-bottom: 0.4em;
            border-bottom: 1px solid #e0e0e0;
        }
        /*
        h3 {
            font-size: 1.3em;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.6em;
        }
        */
        h3 {
            font-family: 'Libre Franklin', sans-serif;  /* ADD THIS */
            font-size: 1.3em;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.6em;
        }
        p {
            margin-bottom: 1.2em;
        }
        
        strong {
            font-weight: 600;
        }
        
        em {
            font-style: italic;
        }
        
        a {
            color: #0066cc;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        /* Code blocks */
        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        
        pre {
            background: #f5f5f5;
            padding: 16px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 1.5em 0;
            border: 1px solid #e0e0e0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        /* Lists */
        ul, ol {
            margin-left: 2em;
            margin-bottom: 1.2em;
        }
        
        li {
            margin-bottom: 0.5em;
        }
        
        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5em 0;
            font-size: 0.95em;
        }
        
        th, td {
            padding: 10px 12px;
            text-align: left;
            border: 1px solid #e0e0e0;
        }
        
        th {
            background: #f5f5f5;
            font-weight: 600;
        }
        
        tr:nth-child(even) {
            background: #fafafa;
        }
        
        /* Figures */
        figure {
            margin: 2em 0;
            text-align: center;
        }
        
        figure img {
            max-width: 100%;
            height: auto;
        }
        
        figcaption {
            margin-top: 0.8em;
            font-size: 0.95em;
            color: #666;
            font-style: italic;
        }
        
        /* Horizontal rules */
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 2em 0;
        }
        
        /* Table of contents (optional - easy to delete) */
        .toc {
            background: #f5f5f5;
            border: 1px solid #e0e0e0;
            border-radius: 4px;
            padding: 20px 24px;
            margin: 2em 0;
        }
        
        .toc h2 {
            font-size: 1.2em;
            margin: 0 0 0.8em 0;
            border: none;
            padding: 0;
        }
        
        .toc ul {
            margin: 0;
            list-style-position: inside;
        }
        
        .toc li {
            margin-bottom: 0.4em;
        }
        
        /* Author note */
        .author-note {
            margin-top: 3em;
            padding-top: 2em;
            border-top: 1px solid #e0e0e0;
            font-style: italic;
            color: #666;
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            body {
                font-size: 17px;
            }
            
            .simple-header {
                flex-direction: column;
                gap: 15px;
                align-items: flex-start;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.4em;
            }
            
            h3 {
                font-size: 1.2em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="simple-header">
            <a href="index.html" class="site-name">Sahar Banisafar</a>
            <nav>
                <a href="index.html">Home</a>
                <a href="index.html#about">About</a>
                <a href="index.html#cv">Resume</a>
            </nav>
        </div>
    </header>

    <main>
        <h1>LoRA: Low-Rank Adaptation for Efficient Fine-Tuning</h1>
        <p class="subtitle">A deep dive into the practice of  low rank parameter efficient fine-tuning</p>

        <!-- TABLE OF CONTENTS - Delete this entire section if you don't want it -->
        <nav class="toc">
            <h2>Contents</h2>
            <ul>
                <li><a href="#landscape">The Fine-Tuning Landscape</a></li>
                <li><a href="#lora">LoRA: Low-Rank Adaptation</a></li>
                <li><a href="#code">Practical Example: LoRA Fine-Tuning on IMDB</a></li>

                <li><a href="#hypothesis">The Low-Rank Hypothesis</a></li>
            </ul>
        </nav>
        <!-- END TABLE OF CONTENTS -->

        <h2 id="landscape">The Fine-Tuning Landscape</h2>


        <p>In the era of large language models, adapting these models to downstream applications has become common practice. These adjustments are done via fine-tuning. Fine tuning can take many forms. While the focus of this post is LoRA, it's important to see where it falls in the big picture. </p>
        <figure>
            <img src="fig1.png" alt="Description of image">
            <figcaption><strong>Figure 1:</strong> Overview of different fine-tuning techniques (Image Source: with additional annotation)  </figcaption>
        </figure>
        
        <p><strong>Full Finetuning</strong></p>
        <p>When models were small enough (hundreds of thousands to a few million parameters) full fine-tuning was the standard approach. All the parameters in the pre-trained model would be updated during training. As model sizes grew to hundreds of billions and even trillions of parameters, this approach became prohibitively expensive in terms of compute, memory, and time, making it inaccessible to most practitioners.</p>
        
        <p><strong>Selective Finetuning</strong></p>
        <p>A natural next step: freeze most of the layers and fine-tune only a subset. For instance, in a 10-layer model, you might freeze the first eight layers and fine-tune only the last two, reducing training to just 20% of the parameters. However, empirical results showed this approach was insufficient. Houlsby et al. (2019) found that for BERT-Large, you'd still need to fine-tune approximately 25% of the model to achieve performance comparable to full fine-tuning. With BERT-Large's 340 million parameters, that still means updating 85 million parameters. Fine-tuning just the top layers is inefficient and doesn't fully solve the compute and memory problem.</p>

        <p><strong>Additive Finetuning</strong></p>
        <p>The question now becomes: <strong>How can we achieve near-full-fine-tuning performance while training orders of magnitude fewer parameters?</strong></p>
        <p>This motivated the development of parameter-efficient fine-tuning (PEFT) methods. Houlsby et al. (2019) introduced <strong>adaptor layers</strong>. This approach inserts small trainable modules into the frozen base model. In a transformer, adapter modules are inserted after the self-attention and feed-forward sublayers. The adapter architecture consists of a down-projection (to reduce dimensionality), a non-linear activation function, and an up-projection (to restore dimensionality). Only these adapter parameters are trained.</p>        
        <p>This approach achieved performance within 0.4% of full fine-tuning while training only 3% of the model's parameters. However, adapter layers introduce a critical drawback: increased inference latency. Because of these additional layers, inference requires extra computation in each forward pass. This is problematic in production systems where low latency is crucial.</p>
        
        <p>There's another class of additive techniques worth mentioning: soft prompting methods such as prefix tuning or prompt tuning. These modify how the model processes the input rather than the architecture itself. We focus on adapters here because they lead most directly to LoRA.</p>
        <p>It's also worth noting that PEFT methods are not only parameter-efficient but also sample-efficient. It can achieve strong performance with just a few thousand training examples, making them practical for low-resource settings.</p>

        <p><strong>Reprameterzation Finetuning</strong></p>
        <p>With the trainable parameters successfully down by order of magnitudes, now time to address the increased inference time. </p>
        <p>Reparameterization methods introduce additional low-rank trainable parameters during training. The key difference from adapters: these parameters can be <strong>merged back into the original weights</strong> at inference time, preserving the base model's architecture. This results in no extra layers, no additional computation, no inference overhead.</p>

        <p>The theoretical groundwork came from Aghajanyan et al. (2020), who showed in Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning that common pre-trained models have a low intrinsic dimensionality. In other words, the weight updates needed during fine-tuning occupy a surprisingly low-dimensional subspace of the full parameter space. If the updates live in a small subspace, we don't need full-rank matrices to represent them.</p>
        <p>This insight opened the door to LoRA and the family of reparameterization methods that followed, including variants like DyLoRA and DoRA. Our focus here remains on the original.</p>




        
        <h2 id="lora">LoRA: Low-Rank Adaptation</h2>


        <p>Now that we’ve seen the big picture of the fine-tuning landscape, let’s focus on LoRA. Introduced by Hu et al. (2021), LoRA was originally applied to transformers but works on any architecture with learnable weight matrices. It is applied to any number matrices in a pretrained model. </p>

        <p>For a pretrained weight matrix W₀ ∈ ℝ<sup>m×n</sup>, we want to learn an matrix ΔW∈ ℝ<sup>m×n</sup>. Think of ΔW as incrementally updating the output to encapsulate task specific knowledge </p>


        <p style="text-align: center; margin: 2em 0;">
        $$W' = W_0 + \frac{\alpha}{r} \Delta W$$
        

        
        <p>Here's the key insight: weight updates during fine-tuning have low intrinsic rank. Even though W₀ is a large matrix, the task-specific changes (ΔW) lie in a much smaller subspace.</p>

        <p>LoRA exploits this by decomposing ΔW into two low-rank matrices </p>

        <p style="text-align: center; margin: 2em 0;">
        $$\Delta W = AB$$
        <p>where:</p>
        <ul>
            <li><strong>A</strong> ∈ ℝ<sup>m×r</sup></li>
            <li><strong>B</strong> ∈ ℝ<sup>r×n</sup></li>
            <li><strong>r</strong> ≪ min(m,n) is the rank</li>

        </ul>



        <figure>
            <img src="fig3.png" alt="LoRA Architecture">
            <figcaption><strong>Figure 2:</strong> LoRA decomposes weight updates into low-rank matrices A and B. The pre-trained weight matrix W₀ remains frozen while the update W<sub>AB</sub> = BA is learned. After training, these merge into a single matrix W' = W₀ + BA with no inference overhead.</figcaption>
        </figure>
        
        
        <p>Instead of learning the full m×n matrix ΔW, we only train the smaller matrices A and B. This decompostions is what allows us to train such few parameters. </p>

        <p>Let’s look at a concrete example with a dimension size of 768 X 768.</p>

        <table>
        <thead>
            <tr>
            <th>Method</th>
            <th>Calculation</th>
            <th>Parameters</th>
            <th>Reduction</th>
            </tr>
        </thead>
        <tbody>
            <tr>
            <td>Full ΔW (d=768)</td>
            <td>768 × 768</td>
            <td>589,824</td>
            <td>—</td>
            </tr>
            <tr>
            <td>LoRA (r=8)</td>
            <td>(768 × 8) + (8 × 768)</td>
            <td>12,288</td>
            <td>98%</td>
            </tr>
        </tbody>
        </table>


        <p>Now this is a major drop.  </p>
        <p> In trainign these , the initialization is such that A is a zero matrix and B is gaussian noise. </p>
        <p></p>

















        <hr>
        <p>LoRA (Hu et al., 2021) provides an elegant solution based on a key insight: <strong>weight updates during fine-tuning have low intrinsic rank</strong>. Rather than updating full weight matrices or adding new layers, LoRA decomposes the weight update ΔW into the product of two low-rank matrices A and B.  These updates can be merged back into the original weights W0 after training, eliminating any inference latency.</p>

        <figure>
            <img src="fig3.png" alt="LoRA Architecture">
            <figcaption><strong>Figure 2:</strong> LoRA decomposes weight updates into low-rank matrices A and B. The pre-trained weight matrix W₀ remains frozen while the update W<sub>AB</sub> = BA is learned. After training, these merge into a single matrix W' = W₀ + BA with no inference overhead.</figcaption>
        </figure>

        <p>For a pre-trained weight matrix W₀ ∈ ℝ<sup>m×n</sup>, LoRA learns two smaller matrices: A ∈ ℝ<sup>m×r</sup> and B ∈ ℝ<sup>r×n</sup>, where the rank r is much smaller than both m and n. Instead of learning a full m×n update, the model only trains these low-rank factors.</p>





        <p>For a pre-trained weight matrix W₀ ∈ ℝm×n, LoRA learns two smaller matrices: </p>
        <ul>
            <li><strong>A</strong> ∈ ℝ<sup>m×r</sup></li>
            <li><strong>B</strong> ∈ ℝ<sup>r×n</sup></li>
            <li><strong>r</strong> ≪ min(m,n) is the rank</li>

        </ul>

        <p>The final weight used during the forward pass becomes:

        <p style="text-align: center; margin: 2em 0;">
        $$W' = W_0 + \frac{\alpha}{r} \Delta W$$
        <p>Where the hyperparameter &alpha; controls for how much the update contributes to the update </p>

        <p>A way to think of ΔW incrementally updating the output to encapsulate task specific knowledge</p>

        <p><strong>For a concrete example:</strong> Consider a 768×768 attention weight matrix in BERT. Full fine-tuning would update all 589,824 parameters. With LoRA at rank r=8, you instead train two matrices: one 768×8 and one 8×768, totaling 12,288 parameters -<strong>a 98% reduction</strong>. This demonstrates the signifcant decrease to cost and memory</p>






        <p>LoRA uniquely solves the fine-tuning trilemma by combining training efficiency, inference speed, and strong performance. But why does this low-rank decomposition work? The answer lies in understanding the intrinsic dimensionality of weight updates during adaptation.</p>




        <p>LoRA exploits this structure by representing the weight update as a product of two low-rank matrices:</p>

        <pre><code>W' = W₀ + ΔW = W₀ + BA</code></pre>

        <p>where:</p>
        <ul>
            <li><strong>W₀</strong> ∈ ℝ<sup>m×n</sup> is the frozen pre-trained weight</li>
            <li><strong>B</strong> ∈ ℝ<sup>m×r</sup></li>
            <li><strong>A</strong> ∈ ℝ<sup>r×n</sup></li>
            <li><strong>r</strong> ≪ min(m,n) is the rank</li>
        </ul>

        <p>The matrix product BA has rank at most r, so we're constraining ΔW to lie in a low-dimensional subspace. During training, W₀ remains frozen—gradients flow only to A and B. After training, we can merge the update: W' = W₀ + BA becomes a single matrix with no computational overhead at inference.</p>
















        <h2 id="code">Practical Example: LoRA Fine-Tuning on IMDB</h2>

        <p>To see LoRA in action, I fine-tuned DistilBERT on the IMDB sentiment classification task using HuggingFace's PEFT library. The full implementation is available in my <a href="your-github-link">GitHub repository</a>.</p>

        <h3>Setup</h3>

        <p>I used a subset of 5,000 training examples and 1,000 test examples (balanced 50/50 positive/negative). The goal was to compare different LoRA configurations to understand the tradeoffs between parameter efficiency and performance.</p>

        <h3>Configuration</h3>

        <p>Here's the core LoRA setup:</p>

        <pre><code class="language-python">from peft import LoraConfig, get_peft_model, TaskType

        # Configure LoRA
        lora_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,       # Sequence classification
            r=8,                               # Rank of decomposition
            lora_alpha=16,                     # Scaling factor (typically 2*r)
            lora_dropout=0.1,                  # Regularization
            target_modules=["q_lin", "v_lin"], # Apply to Q and V matrices
            bias="none",
        )

        # Apply LoRA to the model
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        # trainable params: 294,914 || all params: 66,955,010 || trainable%: 0.44%
        </code></pre>

        <p>With just these few lines, we've converted a standard DistilBERT model (66M parameters) into one where only 295K parameters (0.44%) need to be trained.</p>

        <h3>Experiments</h3>

        <p>I tested several configurations to understand the impact of rank and target modules:</p>

        <!-- YOUR RESULTS TABLE HERE -->
        <table>
        <thead>
            <tr>
            <th>Configuration</th>
            <th>Trainable Params</th>
            <th>Trainable %</th>
            <th>Accuracy</th>
            </tr>
        </thead>
        <tbody>
            <tr>
            <td>Classification Head Only</td>
            <td>1,538</td>
            <td>0.00%</td>
            <td>82.18%</td>
            </tr>
            <tr>
            <td>Q+V (r=4)</td>
            <td>665,858</td>
            <td>0.99%</td>
            <td>92.28%</td>
            </tr>
            <tr>
            <td>Q+V (r=8)</td>
            <td>739,586</td>
            <td>1.10%</td>
            <td>92.48%</td>
            </tr>
            <tr>
            <td>Q+V (r=16)</td>
            <td>887,042</td>
            <td>1.32%</td>
            <td>92.67%</td>
            </tr>
            <tr>
            <td>All Attention (r=8)</td>
            <td>887,042</td>
            <td>1.32%</td>
            <td>93.00%</td>
            </tr>
            <tr style="background: #e8f5ee;">
            <td><strong>All Attention+FFN (r=8)</strong></td>
            <td><strong>1,255,682</strong></td>
            <td><strong>1.88%</strong></td>
            <td><strong>93.32%</strong></td>
            </tr>
            <tr>
            <td>Full Fine-Tuning</td>
            <td>66,955,010</td>
            <td>100.00%</td>
            <td>93.26%</td>
            </tr>
        </tbody>
        </table>

        <p style="margin-top: 1rem; font-size: 0.9em; color: #666;">
        <em>All experiments on NVIDIA A100 GPU with full IMDB dataset (25K train, 25K test).</em>
        </p>
        <h3>Key Findings</h3>

        <p><strong>Rank matters, but shows diminishing returns:</strong></p>
        <ul>
        <li>r=4: 91.0% accuracy with only 147K trainable parameters</li>
        <li>r=8: 91.4% accuracy with 295K parameters</li>
        <li>r=16: 91.5% accuracy with 590K parameters</li>
        </ul>

        <p>The jump from r=4 to r=8 gives a meaningful improvement, but doubling to r=16 yields minimal gains.</p>

        <p><strong>Target module selection:</strong></p>
        <ul>
        <li>Q+V only (r=8): Strong baseline at 91.4% accuracy</li>
        <li>All 4 attention matrices (r=8): Marginal improvement to 91.5%</li>
        <li>Adding FFN layers (r=8): Best performance at 91.9%, but 2.7x more parameters</li>
        </ul>

        <p><strong>Baseline comparison:</strong></p>
        <p>Training only the classification head (1.5K parameters) achieved 88.5% accuracy. LoRA configurations with 147K-295K parameters all significantly outperformed this, demonstrating that even minimal fine-tuning of the transformer layers matters.</p>

        <h3>The Sweet Spot</h3>

        <p>For this task, <strong>Q+V with r=8</strong> offers the best tradeoff: 91.4% accuracy with only 0.44% of parameters trainable. Adding FFN layers improves performance slightly but requires 2.7x more parameters—worthwhile for some applications, but not necessary for strong results.</p>

        <h3>Implementation Notes</h3>

        <ul>
        <li><strong>Learning rate:</strong> LoRA typically works well with higher learning rates (3e-4) compared to full fine-tuning (2e-5)</li>
        <li><strong>Training time:</strong> All LoRA configurations trained in 15-22 minutes on a single GPU, comparable to baseline</li>
        <li><strong>Merging:</strong> After training, the LoRA matrices can be merged back into the original weights for zero-overhead inference</li>
        </ul>

        <p>The full training code, including data preprocessing and evaluation, is available on <a href="your-github-link">GitHub</a>.</p>





        <h2 id="hypothesis"> IGNORE ALL BELOW The Low-Rank Hypothesis</h2>



        
        <p><strong>Why LoRA has become the dominant PEFT technique:</strong></p>
        <ul>
            <li><strong>Parameter efficiency:</strong> Trains only 0.1-1% of model parameters, even better than adapter layers' 3-4%</li>
            <li><strong>Zero inference overhead:</strong> The low-rank matrices merge into the original weights after training, so the deployed model has identical architecture and latency to the base model</li>
            <li><strong>Universal applicability:</strong> Works on any model architecture that uses matrix multiplication—not limited to transformers or language models</li>
            <li><strong>Strong performance:</strong> Achieves 95-99% of full fine-tuning performance across diverse tasks</li>
            <li><strong>Sample efficiency:</strong> Delivers strong results even with limited training data</li>
        </ul>


        <h3>Understanding Intrinsic Rank</h3>
        <p>The <strong>rank</strong> of a matrix is the dimension of its column space—intuitively, it tells us how many independent directions of variation the matrix contains. A full-rank m×n matrix has rank min(m,n), meaning it spans the entire space. A low-rank matrix, by contrast, lies in a much smaller subspace.</p>

        <p>LoRA's central hypothesis is this: <strong>during fine-tuning on a downstream task, the weight update ΔW = W<sub>new</sub> - W<sub>pretrained</sub> has low intrinsic rank</strong>. This means that even though ΔW is technically an m×n matrix, most of its structure can be captured by a rank-r approximation where r ≪ min(m,n).</p>

        <h3>Why Weight Updates Are Low-Rank</h3>
        <p>This hypothesis isn't arbitrary—it's grounded in how pre-trained models work. A model pre-trained on massive datasets has already learned rich, general representations. The weight matrices W₀ encode a vast amount of knowledge about language structure, visual patterns, or whatever domain the model was trained on.</p>

        <p>When we fine-tune for a specific task, we're not learning everything from scratch. We're making targeted adjustments to redirect this existing knowledge toward the new objective. These adjustments don't require full-rank changes across all dimensions—they happen along a small number of key directions in the parameter space.</p>

        <p>Consider an analogy: If you're fluent in Spanish and learning Portuguese, you don't need to relearn the entire language system. You adjust along specific dimensions (pronunciation differences, some vocabulary, certain grammatical patterns) while keeping the bulk of your knowledge intact. Similarly, fine-tuning adjusts pre-trained weights along a low-dimensional subspace.</p>

        <h3>Empirical Evidence</h3>
        <p>The LoRA paper provides compelling empirical support for this hypothesis. Experiments on GPT-3 (175B parameters) showed that ranks as low as r=1 or r=2 often suffice for effective adaptation. Even for challenging tasks, r=4 or r=8 consistently matched full fine-tuning performance.</p>

        <p>This remarkable finding tells us that the true dimensionality of task-specific adaptation is far smaller than the model's parameter count suggests. A 768×768 weight matrix (589,824 parameters) can be effectively adapted with a rank-8 update (12,288 parameters)—the useful adaptation occurs in less than 1% of the parameter space.</p>

        <hr>

        <h2>References</h2>
        <p>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. <em>arXiv preprint arXiv:2106.09685</em>.</p>

        <p>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... & Gelly, S. (2019). Parameter-efficient transfer learning for NLP. <em>International Conference on Machine Learning</em>, 2790-2799.</p>

        <p>Li, X. L., & Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. <em>arXiv preprint arXiv:2101.00190</em>.</p>

        <p>Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. <em>arXiv preprint arXiv:2104.08691</em>.</p>

        <p>Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. <em>arXiv preprint arXiv:2305.14314</em>.</p>

        <p>Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., & Zhao, T. (2023). AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning. <em>arXiv preprint arXiv:2303.10512</em>.</p>

        <p class="author-note">Sahar Banisafar is a data scientist with a mathematics background and 6 years of production ML experience. She writes about the intersection of theory and practice in modern machine learning.</p>
    </main>
</body>
</html>
