<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A deep dive into LoRA (Low-Rank Adaptation) - mathematical foundations and practical implementation">
    <meta name="author" content="Sahar Banisafar">
    <link href="https://fonts.googleapis.com/css2?family=Libre+Franklin:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <title>LoRA: Low-Rank Adaptation for Efficient Fine-Tuning</title>
    
    <!-- Math rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;
            
            font-size: 18px;
            line-height: 1.7;
            color: #333;
            background: #fff;
        }
        
        /* Header styles */
        header {
            border-bottom: 1px solid #e0e0e0;
            padding: 20px 0;
            margin-bottom: 40px;
        }
        
        .simple-header {
            max-width: 750px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .site-name {
            font-size: 20px;
            font-weight: 700;
            color: #333;
            text-decoration: none;
        }
        /*
        .site-name {
            font-family: 'Libre Franklin', sans-serif;  /* ADD THIS */
            font-size: 20px;
            font-weight: 700;
            color: #333;
            text-decoration: none;
        }
        */
        
        .site-name:hover {
            color: #0066cc;
        }
        
        header nav {
            display: flex;
            gap: 20px;
        }
        
        header nav a {
            color: #666;
            text-decoration: none;
            font-size: 16px;
        }
        
        header nav a:hover {
            color: #0066cc;
        }
        
        /* Main content */
        main {
            max-width: 750px;
            margin: 0 auto;
            padding: 0 20px 80px;
        }
        
        h1 {
            font-family: 'Libre Franklin', sans-serif;
            font-size: 2.2em;
            font-weight: 600;
            margin-bottom: 0.3em;
            line-height: 1.2;
        }
        
        .subtitle {
            font-style: italic;
            color: #666;
            margin-bottom: 2em;
        }
        
        h2 {
            font-family: 'Libre Franklin', sans-serif;  /* ADD THIS */
            font-size: 1.6em;
            font-weight: 600;
            margin-top: 2em;
            margin-bottom: 0.8em;
            padding-bottom: 0.4em;
            border-bottom: 1px solid #e0e0e0;
        }
        /*
        h3 {
            font-size: 1.3em;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.6em;
        }
        */
        h3 {
            font-family: 'Libre Franklin', sans-serif;  /* ADD THIS */
            font-size: 1.3em;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.6em;
        }
        p {
            margin-bottom: 1.2em;
        }
        
        strong {
            font-weight: 600;
        }
        
        em {
            font-style: italic;
        }
        
        a {
            color: #0066cc;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        /* Code blocks */
        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-size: 0.9em;
        }
        
        pre {
            background: #f5f5f5;
            padding: 16px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 1.5em 0;
            border: 1px solid #e0e0e0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        /* Lists */
        ul, ol {
            margin-left: 2em;
            margin-bottom: 1.2em;
        }
        
        li {
            margin-bottom: 0.5em;
        }
        
        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5em 0;
            font-size: 0.95em;
        }
        
        th, td {
            padding: 10px 12px;
            text-align: left;
            border: 1px solid #e0e0e0;
        }
        
        th {
            background: #f5f5f5;
            font-weight: 600;
        }
        
        tr:nth-child(even) {
            background: #fafafa;
        }
        
        /* Figures */
        figure {
            margin: 2em 0;
            text-align: center;
        }
        
        figure img {
            max-width: 100%;
            height: auto;
        }
        
        figcaption {
            margin-top: 0.8em;
            font-size: 0.95em;
            color: #666;
            font-style: italic;
        }
        
        /* Horizontal rules */
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 2em 0;
        }
        
        /* Table of contents (optional - easy to delete) */
        .toc {
            background: #f5f5f5;
            border: 1px solid #e0e0e0;
            border-radius: 4px;
            padding: 20px 24px;
            margin: 2em 0;
        }
        
        .toc h2 {
            font-size: 1.2em;
            margin: 0 0 0.8em 0;
            border: none;
            padding: 0;
        }
        
        .toc ul {
            margin: 0;
            list-style-position: inside;
        }
        
        .toc li {
            margin-bottom: 0.4em;
        }
        
        /* Author note */
        .author-note {
            margin-top: 3em;
            padding-top: 2em;
            border-top: 1px solid #e0e0e0;
            font-style: italic;
            color: #666;
        }
        
        /* Responsive */
        @media (max-width: 768px) {
            body {
                font-size: 17px;
            }
            
            .simple-header {
                flex-direction: column;
                gap: 15px;
                align-items: flex-start;
            }
            
            h1 {
                font-size: 1.8em;
            }
            
            h2 {
                font-size: 1.4em;
            }
            
            h3 {
                font-size: 1.2em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="simple-header">
            <a href="index.html" class="site-name">Sahar Banisafar</a>
            <nav>
                <a href="index.html">Home</a>
                <a href="index.html#about">About</a>
                <a href="index.html#cv">CV</a>
            </nav>
        </div>
    </header>

    <main>
        <h1>LoRA: Low-Rank Adaptation for Efficient Fine-Tuning</h1>
        <p class="subtitle">A big picture view and deep dive into the practice of  low rank parameter effiecitn fine-tuning</p>

        <!-- TABLE OF CONTENTS - Delete this entire section if you don't want it -->
        <nav class="toc">
            <h2>Contents</h2>
            <ul>
                <li><a href="#landscape">FT Landscape and Challenges</a></li>
                <li><a href="#bigpic">Big Picture</a></li>
                <li><a href="#lora">LoRA: Low-Rank Adaptation</a></li>
                <li><a href="#hypothesis">The Low-Rank Hypothesis</a></li>
            </ul>
        </nav>
        <!-- END TABLE OF CONTENTS -->

        <h2 id="landscape">The Fine-Tuning Landscape</h2>


        <p>In the era of large language models, adapting these models to downstream applications has become common practice. These adjustments are done via fine-tuning. Fine tuning can take many forms. While the focus of this post is LoRA, it's important to see where it falls in the big picture of fine tuning. </p>
        <figure>
            <img src="fig1.png" alt="Description of image">
            <figcaption><strong>Figure 1:</strong> Overview of differnet fine tuning techniques (Image Source: with additional annotation)  </figcaption>
        </figure>
        
        <p><strong>Full Finetuning</strong></p>
        <p>When models were small enough, (hundreds of thousands to a few million parameters) full fine-tuning was the standard approach. All the parameters in the pre-trained model would be updated. As model sizes grew to hundreds of billions to trillions of parameters, this approach became prohibitively expensive in terms of compute, memory, and time, making it inaccessible to most practitioners</p>
        
        <p><strong>Selective Finetuning</strong></p>
       <p>A natural next step: selective fine-tuning. Freeze most of the layers and fine-tune only a subset. For instance, in a 10-layer model, you might freeze the first eight layers and fine-tune only the last two, reducing training to only 20% of the parameters. However, empirical results showed this approach was inadequate. The Houlsby et al. (2019) paper found that for BERT-Large, you'd still need to fine-tune approximately 25% of the model to achieve performance comparable to full fine-tuning. BERT-Large has 340 million parameters meaning 85 million parameters still need updating. Fine-tuning just the top layers is inefficient and doesn't fully solve the compute and memory problem.</p>

        <p><strong>Additive Finetuning</strong></p>
        <p>The question now becomes: <strong>How can we achieve near-full-fine-tuning performance while training orders of magnitude fewer parameters?</strong></p>
        <p>This motivated the development of parameter-efficient fine-tuning (PEFT) methods. Houlsby et al. (2019) introduced <strong>adaptor layers</strong>, an additive approach that inserts small trainable modules into the frozen base model. In a transformer, adapter modules are inserted after the attention and feed-forward layers. Each adapter consists of a down-projection (to reduce dimensionality), an activation function, and an up-projection (to restore dimensionality). Only these adapter parameters are trained.</p>        
        <p>This approach achieved performance within 0.4% of full fine-tuning while training only 3% of the model's parameters. However, adapter layers introduce a critical drawback: increased inference latency. Because the adapters are additional layers in the model during inference, each forward pass requires extra computation.</p>
        <p>(There is another type of additive technique called Soft Prompting. This differs in that the additional tunable parameters are added to the input and become special trainable tokene. There are other additive techniques worth mentioning, e.g. prompt tuning, prefix tuning, and IA3. We focus on adapters here because they most directly lead us to LoRA. But these other methods aren't forgotten.)</p>
        <p>It's worth noting that PEFT methods are not only parameter-efficient but also sample-efficient—many can achieve strong performance with just a few thousand training examples, making them practical for low-resource settings.</p>

        
        <p><strong>Reprameterzation Finetuning</strong></p>
        <p>With the trainable parameters successfully down by order of magnitudes, now time to address the increased inference time. </p>
        <p>Reparameterization introduce additional low rank trainable parameters during training which can be <strong>merged</strong> into the original model during inference. This allows for us to not have additional layers that must be computed. </p>
        <p>The paper intrinsic SAID did the pioneering work in introducing “ it said BLAH BLAH “ and we well explore this a bit further. By far the most popular Reparameterizaiton PEFT is LoRA and we will take a deep dive below. </p>
        <p>There are a host of techniques that have come up DyLoRA, DoRA, but the focus here will remain.</p>








        
        <h2 id="lora">LoRA: Low-Rank Adaptation</h2>
        <p>LoRA (Hu et al., 2021) provides an elegant solution based on a key insight: <strong>weight updates during fine-tuning have low intrinsic rank</strong>. Rather than updating full weight matrices or adding new layers, LoRA decomposes the weight update ΔW into the product of two low-rank matrices A and B.  These updates can be merged back into the original weights W0 after training, eliminating any inference latency.</p>

        <figure>
            <img src="fig3.png" alt="LoRA Architecture">
            <figcaption><strong>Figure 1:</strong> LoRA decomposes weight updates into low-rank matrices A and B. The pre-trained weight matrix W₀ remains frozen while the update W<sub>AB</sub> = BA is learned. After training, these merge into a single matrix W' = W₀ + BA with no inference overhead.</figcaption>
        </figure>

        <p>For a pre-trained weight matrix W₀ ∈ ℝ<sup>m×n</sup>, LoRA learns two smaller matrices: A ∈ ℝ<sup>m×r</sup> and B ∈ ℝ<sup>r×n</sup>, where the rank r is much smaller than both m and n (typically r = 8). Instead of learning a full m×n update, the model only trains these low-rank factors.</p>





        <p>For a pre-trained weight matrix W₀ ∈ ℝm×n, LoRA learns two smaller matrices: </p>
        <ul>
            <li><strong>A</strong> ∈ ℝ<sup>m×r</sup></li>
            <li><strong>B</strong> ∈ ℝ<sup>r×n</sup></li>
            <li><strong>r</strong> ≪ min(m,n) is the rank</li>

        </ul>
        <p>The final weight used during the forward pass becomes: W' = W₀ +Alpha/r ΔW</p>
        <p>Where alpha is a control for how much the update contributes to the original model </p>


        <p><strong>For a concrete example:</strong> Consider a 768×768 attention weight matrix in BERT. Full fine-tuning would update all 589,824 parameters. With LoRA at rank r=8, you instead train two matrices: one 768×8 and one 8×768, totaling 12,288 parameters -<strong>a 98% reduction</strong>. This demonstrates the signifcant decrease to cost and memory</p>






        <p>LoRA uniquely solves the fine-tuning trilemma by combining training efficiency, inference speed, and strong performance. But why does this low-rank decomposition work? The answer lies in understanding the intrinsic dimensionality of weight updates during adaptation.</p>




        <p>LoRA exploits this structure by representing the weight update as a product of two low-rank matrices:</p>

        <pre><code>W' = W₀ + ΔW = W₀ + BA</code></pre>

        <p>where:</p>
        <ul>
            <li><strong>W₀</strong> ∈ ℝ<sup>m×n</sup> is the frozen pre-trained weight</li>
            <li><strong>B</strong> ∈ ℝ<sup>m×r</sup></li>
            <li><strong>A</strong> ∈ ℝ<sup>r×n</sup></li>
            <li><strong>r</strong> ≪ min(m,n) is the rank</li>
        </ul>

        <p>The matrix product BA has rank at most r, so we're constraining ΔW to lie in a low-dimensional subspace. During training, W₀ remains frozen—gradients flow only to A and B. After training, we can merge the update: W' = W₀ + BA becomes a single matrix with no computational overhead at inference.</p>















        
        <p><strong>Why LoRA has become the dominant PEFT technique:</strong></p>
        <ul>
            <li><strong>Parameter efficiency:</strong> Trains only 0.1-1% of model parameters, even better than adapter layers' 3-4%</li>
            <li><strong>Zero inference overhead:</strong> The low-rank matrices merge into the original weights after training, so the deployed model has identical architecture and latency to the base model</li>
            <li><strong>Universal applicability:</strong> Works on any model architecture that uses matrix multiplication—not limited to transformers or language models</li>
            <li><strong>Strong performance:</strong> Achieves 95-99% of full fine-tuning performance across diverse tasks</li>
            <li><strong>Sample efficiency:</strong> Delivers strong results even with limited training data</li>
        </ul>


        <h2 id="hypothesis">The Low-Rank Hypothesis</h2>

        <h3>Understanding Intrinsic Rank</h3>
        <p>The <strong>rank</strong> of a matrix is the dimension of its column space—intuitively, it tells us how many independent directions of variation the matrix contains. A full-rank m×n matrix has rank min(m,n), meaning it spans the entire space. A low-rank matrix, by contrast, lies in a much smaller subspace.</p>

        <p>LoRA's central hypothesis is this: <strong>during fine-tuning on a downstream task, the weight update ΔW = W<sub>new</sub> - W<sub>pretrained</sub> has low intrinsic rank</strong>. This means that even though ΔW is technically an m×n matrix, most of its structure can be captured by a rank-r approximation where r ≪ min(m,n).</p>

        <h3>Why Weight Updates Are Low-Rank</h3>
        <p>This hypothesis isn't arbitrary—it's grounded in how pre-trained models work. A model pre-trained on massive datasets has already learned rich, general representations. The weight matrices W₀ encode a vast amount of knowledge about language structure, visual patterns, or whatever domain the model was trained on.</p>

        <p>When we fine-tune for a specific task, we're not learning everything from scratch. We're making targeted adjustments to redirect this existing knowledge toward the new objective. These adjustments don't require full-rank changes across all dimensions—they happen along a small number of key directions in the parameter space.</p>

        <p>Consider an analogy: If you're fluent in Spanish and learning Portuguese, you don't need to relearn the entire language system. You adjust along specific dimensions (pronunciation differences, some vocabulary, certain grammatical patterns) while keeping the bulk of your knowledge intact. Similarly, fine-tuning adjusts pre-trained weights along a low-dimensional subspace.</p>

        <h3>Empirical Evidence</h3>
        <p>The LoRA paper provides compelling empirical support for this hypothesis. Experiments on GPT-3 (175B parameters) showed that ranks as low as r=1 or r=2 often suffice for effective adaptation. Even for challenging tasks, r=4 or r=8 consistently matched full fine-tuning performance.</p>

        <p>This remarkable finding tells us that the true dimensionality of task-specific adaptation is far smaller than the model's parameter count suggests. A 768×768 weight matrix (589,824 parameters) can be effectively adapted with a rank-8 update (12,288 parameters)—the useful adaptation occurs in less than 1% of the parameter space.</p>

        <hr>

        <h2>References</h2>
        <p>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. <em>arXiv preprint arXiv:2106.09685</em>.</p>

        <p>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., ... & Gelly, S. (2019). Parameter-efficient transfer learning for NLP. <em>International Conference on Machine Learning</em>, 2790-2799.</p>

        <p>Li, X. L., & Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. <em>arXiv preprint arXiv:2101.00190</em>.</p>

        <p>Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. <em>arXiv preprint arXiv:2104.08691</em>.</p>

        <p>Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. <em>arXiv preprint arXiv:2305.14314</em>.</p>

        <p>Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., & Zhao, T. (2023). AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning. <em>arXiv preprint arXiv:2303.10512</em>.</p>

        <p class="author-note">Sahar Banisafar is a data scientist with a mathematics background and 6 years of production ML experience. She writes about the intersection of theory and practice in modern machine learning.</p>
    </main>
</body>
</html>
